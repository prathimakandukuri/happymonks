{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c1773d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5f1f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fa84c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c47805e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class FCLayer:\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.input = None\n",
    "        self.output = None \n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa28650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "        self.input = None\n",
    "        self.output = None \n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1b75330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36dac029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "355c78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a5d1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d015fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6e92930",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf1a63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3146f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62adc8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "print('Training the model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6c1a7c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   error=0.039714\n",
      "epoch 2/10   error=0.019463\n",
      "epoch 3/10   error=0.015540\n",
      "epoch 4/10   error=0.013287\n",
      "epoch 5/10   error=0.011739\n",
      "epoch 6/10   error=0.010673\n",
      "epoch 7/10   error=0.009794\n",
      "epoch 8/10   error=0.009084\n",
      "epoch 9/10   error=0.008481\n",
      "epoch 10/10   error=0.007998\n"
     ]
    }
   ],
   "source": [
    "net.fit(x_train, y_train, epochs=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4afbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = net.predict(x_train)\n",
    "y_train_pred = np.argmax(y_train_pred,axis=-1).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf52e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Performance :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      5923\n",
      "           1       0.99      0.98      0.99      6742\n",
      "           2       0.98      0.97      0.97      5958\n",
      "           3       0.97      0.95      0.96      6131\n",
      "           4       0.97      0.98      0.97      5842\n",
      "           5       0.94      0.97      0.96      5421\n",
      "           6       0.98      0.98      0.98      5918\n",
      "           7       0.98      0.97      0.98      6265\n",
      "           8       0.96      0.95      0.96      5851\n",
      "           9       0.95      0.95      0.95      5949\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = np.argmax(y_train,axis=-1)\n",
    "print('Training Data Performance :\\n')\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d38011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = net.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_pred,axis=-1).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79e034f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Performance :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.97      0.94      0.96      1032\n",
      "           3       0.93      0.95      0.94      1010\n",
      "           4       0.95      0.96      0.96       982\n",
      "           5       0.93      0.96      0.94       892\n",
      "           6       0.96      0.97      0.97       958\n",
      "           7       0.97      0.94      0.96      1028\n",
      "           8       0.95      0.93      0.94       974\n",
      "           9       0.94      0.92      0.93      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = np.argmax(y_test,axis=-1)\n",
    "print('Test Data Performance :\\n')\n",
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b72a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
